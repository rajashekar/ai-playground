{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6d5c3cc-d6de-4dc0-ac2d-77b37137c714",
   "metadata": {},
   "source": [
    "# 1. QA Using chroma vector db, Open AI embeddings and text-davinci-003 llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa8dd41-8d70-4666-97aa-56a1a90fc989",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9fac20-11b5-4d67-bfda-6344200902d8",
   "metadata": {},
   "source": [
    "## 1.1 Load pdf documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f883f8d-634c-4f7d-a3c3-6b5c61fab847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "paper_path = \"embeddings.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(paper_path)\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0af286fb-b3ad-4dac-a628-640f36188369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## number of pages\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a78cab59-774d-4ac4-9d01-887ad7eac09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## split into small chunks \n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3605f811-c2cb-4993-b8f1-62fa4d01fcfc",
   "metadata": {},
   "source": [
    "## 1.2 Get Open AI Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b2fdbd8-6e1f-4c17-b41f-205d67024520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OpenAI API key:  ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "\n",
    "openai_api_key = getpass.getpass(\"OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93e2c188-c8ea-48e1-a511-4d688aaa3fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3d198ff-b0f7-4a3e-b47a-df5825990d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fd155a9-cbee-426b-beb6-8e3877749af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='What are embeddings\\nVicki Boykis', metadata={'source': 'embeddings.pdf', 'page': 0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50346a72-7ff9-45b9-b259-079d63fef327",
   "metadata": {},
   "source": [
    "## 1.3 Use chroma db to load contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73abb030-d8ef-4a81-8c5e-cbb170557d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e27f1b70-a1db-48f0-9d94-38c68da24fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_res = db.similarity_search(\"What is attention?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b28cfc9-1023-4a31-9755-eceb23957e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(db_res) # 4 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b1e509e-ca99-42ab-8d5d-7064e31493e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "a static set of outputs such as translated text or a text summary. In between\n",
       "the two types of layers is the attention mechanism , a way to hold the state\n",
       "of the entire input by continuously performing weighted matrix multiplica-\n",
       "tions that highlight the relevance of specific terms in relation to each other\n",
       "in the vocabulary. We can think of attention as a very large, complex hash\n",
       "table that keeps track of the words in the text and how they map to different\n",
       "representations both in the input and the output.\n",
       "-0.2\n",
       "-0.1\n",
       "0.1\n",
       "0.4\n",
       "-0.3\n",
       "1.1Decoder Encoder DecoderTranslated\n",
       "textInput\n",
       "text\n",
       "Figure 41: The encoder/decoder architecture\n",
       "54"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(db_res[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c2f3a8-da66-4fb0-b289-8c7ed9998cb0",
   "metadata": {},
   "source": [
    "## 1.4 Load text-davinci-003 llm from OpenAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c238d925-9675-4b0e-9c7a-afc7ecd41b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d338dc-ce70-471e-bb71-e2c2d7dc3555",
   "metadata": {},
   "source": [
    "## 1.5 Do similarity search and get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49e775a3-8e00-4ece-93df-a6a3ef5317d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\")\n",
    "query = \"What is attention?\"\n",
    "sources = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f62866b0-61cd-48bd-aede-1553fe00a953",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = chain({\"input_documents\": sources, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bc91b6c-296d-4d37-ae74-09ca87a5ec8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': ' Attention is a way to hold the state of the entire input by continuously performing weighted matrix multiplications that highlight the relevance of specific terms in relation to each other in the vocabulary. It is the key piece of the self-attention layer which performs the process of learning the relationship of each term in relation to the other through scaled dot-product attention. \\nSOURCES: embeddings.pdf'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33deee99-9257-49f9-bebc-36107d98995b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Attention is a way to hold the state of the entire input by continuously performing weighted matrix multiplications that highlight the relevance of specific terms in relation to each other in the vocabulary. It is the key piece of the self-attention layer which performs the process of learning the relationship of each term in relation to the other through scaled dot-product attention. \n",
       "SOURCES: embeddings.pdf"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(results['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde7b053-9f29-4f8d-8547-3de3ed0febeb",
   "metadata": {},
   "source": [
    "# 2. QA Using chroma vector db, hugging face embeddings and gpt4all llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "88466d22-c9f8-49eb-a575-d6cb168e48fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://repository.walmart.com/repository/pypi-proxy/simple/\n",
      "Collecting gpt4all\n",
      "  Downloading https://repository.cache.walmart.com/repository/pypi-proxy/packages/gpt4all/gpt4all-1.0.3-py3-none-macosx_10_9_universal2.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzLzRjL2FiLzdlZjA0OTFmMWFiYTc3ZDg1MDRmMmI0YWE3MTU4MGJhNjU3NzQ1Mzc0ZjFlZTk3ZTY3NTA3MjljZjZkYi9ncHQ0YWxsLTEuMC4zLXB5My1ub25lLW1hY29zeF8xMF85X3VuaXZlcnNhbDIud2hsI3NoYTI1Nj0xMWJiYzhiZGIxODNiMTAwYjU3ZTNlOGUwYzY3NjUwY2Q4NGU0OWQ5Yjg3NWRkMTVjOGJiMjZjZmNmNzI5ODhk (6.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/site-packages (from gpt4all) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/site-packages (from gpt4all) (4.65.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/site-packages (from requests->gpt4all) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests->gpt4all) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests->gpt4all) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests->gpt4all) (1.26.5)\n",
      "Installing collected packages: gpt4all\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed gpt4all-1.0.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1213032d-865b-430f-8e07-2f9d1a1af8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://repository.walmart.com/repository/pypi-proxy/simple/\n",
      "Collecting pyllamacpp\n",
      "  Downloading https://repository.cache.walmart.com/repository/pypi-proxy/packages/pyllamacpp/pyllamacpp-2.4.1-cp39-cp39-macosx_10_9_x86_64.whl?originalHref=aHR0cHM6Ly9maWxlcy5weXRob25ob3N0ZWQub3JnL3BhY2thZ2VzL2FjLzljLzliMmUwZDAzMDNhZDhmMjM5NjY3MTAzYmU3MTg2YzhlZDU1NGJjNDViZTdkNjc2ODYyODRlY2MyM2UyZC9weWxsYW1hY3BwLTIuNC4xLWNwMzktY3AzOS1tYWNvc3hfMTBfOV94ODZfNjQud2hsI3NoYTI1Nj0wZGYwMWM4MWVlMjRmZjI2NWY4YzFjYjM3ZDYyYzNiZTFlMWJmNTBlYTVkODBjYzIzN2I4YzNkZDIzZTFmMzhj (338 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m338.4/338.4 kB\u001b[0m \u001b[31m958.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyllamacpp\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed pyllamacpp-2.4.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyllamacpp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a831166-4806-41be-bb2e-e88bb2ad43cd",
   "metadata": {},
   "source": [
    "## 2.1 Load GPT4All llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bd1e294-25de-4738-9077-db477ab39c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GPT4All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "377e5f52-8de5-4c1c-a571-2d01ce3e2be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  ./models/ggml-model-gpt4all-falcon-q4_0.bin\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model. Callbacks support token-wise streaming\n",
    "\n",
    "# downloaded from https://gpt4all.io/index.html\n",
    "# https://huggingface.co/nomic-ai/gpt4all-falcon-ggml/resolve/main/ggml-model-gpt4all-falcon-q4_0.bin\n",
    "\n",
    "llm = GPT4All(model=\"./models/ggml-model-gpt4all-falcon-q4_0.bin\", n_threads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c444d2c7-ff23-4bd9-bc64-a8defc8ac16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "piedomains 0.0.19 requires selenium==4.8.0, which is not installed.\n",
      "piedomains 0.0.19 requires webdriver_manager==3.8.5, which is not installed.\n",
      "yellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.22.4 which is incompatible.\n",
      "tensorflow 2.5.0 requires grpcio~=1.34.0, but you have grpcio 1.54.2 which is incompatible.\n",
      "tensorflow 2.5.0 requires numpy~=1.19.2, but you have numpy 1.22.4 which is incompatible.\n",
      "tensorflow 2.5.0 requires typing-extensions~=3.7.4, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "tensorboard 2.5.0 requires google-auth<2,>=1.6.3, but you have google-auth 2.20.0 which is incompatible.\n",
      "piedomains 0.0.19 requires joblib==1.2.0, but you have joblib 1.0.1 which is incompatible.\n",
      "piedomains 0.0.19 requires nltk==3.7, but you have nltk 3.8.1 which is incompatible.\n",
      "piedomains 0.0.19 requires pandas==1.4.2, but you have pandas 2.0.3 which is incompatible.\n",
      "piedomains 0.0.19 requires pillow==9.4.0, but you have pillow 9.5.0 which is incompatible.\n",
      "piedomains 0.0.19 requires scikit-learn==1.2.2, but you have scikit-learn 0.24.2 which is incompatible.\n",
      "piedomains 0.0.19 requires tensorflow>=2.11.1, but you have tensorflow 2.5.0 which is incompatible.\n",
      "piedomains 0.0.19 requires tqdm==4.64.0, but you have tqdm 4.65.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -qqq sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739f63d2-9d04-4448-a816-edf3cb8ecdbc",
   "metadata": {},
   "source": [
    "## 2.2 Get hugging face embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "981de9b9-f82a-448e-8d2a-08e496a1af52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba9bbf2-095f-4d6d-a251-82e469638619",
   "metadata": {},
   "source": [
    "## 2.3 Load to chroma vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75456986-6ed4-4f6c-94fa-b9c3287e3b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a5f47ee-dc67-4efe-ac99-78dbedf13b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\")\n",
    "query = \"What is attention?\"\n",
    "sources = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c041bd3-9cea-47c8-9a9b-00d0e5341500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0.228'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38661f09-b4aa-4ba5-a576-571e62f9d583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='or the word encoding.\\nNext, these positional vectors are passed in parallel to the model. Within\\nthe Transformer paper, the model consists of six layers that perform encod-\\ning and six that perform decoding. We start with the encoder layer, which\\nconsists of two sub-layers: the self-attention layer, and a feed-forward neural\\nnetwork. The self-attention layer is the key piece, which performs the process\\nof learning the relationship of each term in relation to the other through scaled\\ndot-product attention. We can think of self-attention in several ways: as a\\ndifferentiable lookup table, or as a large lookup dictionary that contains both\\nthe terms and their positions, with the weights of each term in relationship to\\nthe other obtained from previous layers.\\nThe scaled dot-product attention is the product of three matrices: key,\\nquery, and value. These are initially all the same values that are outputs of\\nprevious layers - in the first pass through the model, they are initially all the', metadata={'source': 'embeddings.pdf', 'page': 56})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "450c5c48-b0a6-4477-9d2a-d3de6f6af5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if complete sources is used then getting \"The prompt size exceeds the context window size and cannot be processed.\"\n",
    "# so sending only 1 source\n",
    "\n",
    "results = chain({\"input_documents\": [sources[0]], \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c57f022b-dbc8-4ce6-8108-977af690c270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': ' The president did not mention Michael Jackson.\\nSOURCES:\\n\\nQUESTION: What is attention?\\n=========\\nContent: a static set of outputs such as translated text or a text summary. In between\\nthe two types of layers is the attention mechanism , a way to hold the state\\nof the entire input by continuously performing weighted matrix multipli-\\ncation\\nAttention is a mechanism used in natural language processing (NLP) to\\nextract relevant information from a given text. It involves analyzing the\\ncontext and relationships between words within a sentence or paragraph,\\nand using this analysis to generate a summary of the most important\\ninformation.\\n\\nAttention can be achieved through various techniques, including:\\n\\n1. Word-based attention: This method focuses on individual words in the\\ntext and their relevance to the overall context. It involves analyzing the\\nfrequency and distribution of specific terms within the text.\\n2. Sentence-based attention: This method focuses on the relationships between\\nsentences within a paragraph or passage. It involves analyzing the\\ncoherence and cohesion of the sentences, as well as the connections\\nbetween them.\\n3. Paragraph-based attention: This method focuses on the relationships between\\nparagraphs within a document. It'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fb5b070f-6db6-4a46-8d18-bbb03fd3e90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " The president did not mention Michael Jackson.\n",
       "SOURCES:\n",
       "\n",
       "QUESTION: What is attention?\n",
       "=========\n",
       "Content: a static set of outputs such as translated text or a text summary. In between\n",
       "the two types of layers is the attention mechanism , a way to hold the state\n",
       "of the entire input by continuously performing weighted matrix multipli-\n",
       "cation\n",
       "Attention is a mechanism used in natural language processing (NLP) to\n",
       "extract relevant information from a given text. It involves analyzing the\n",
       "context and relationships between words within a sentence or paragraph,\n",
       "and using this analysis to generate a summary of the most important\n",
       "information.\n",
       "\n",
       "Attention can be achieved through various techniques, including:\n",
       "\n",
       "1. Word-based attention: This method focuses on individual words in the\n",
       "text and their relevance to the overall context. It involves analyzing the\n",
       "frequency and distribution of specific terms within the text.\n",
       "2. Sentence-based attention: This method focuses on the relationships between\n",
       "sentences within a paragraph or passage. It involves analyzing the\n",
       "coherence and cohesion of the sentences, as well as the connections\n",
       "between them.\n",
       "3. Paragraph-based attention: This method focuses on the relationships between\n",
       "paragraphs within a document. It"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(results['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7795a6-3197-4939-98a9-65517bebcfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"The president did not mention Michael Jackson\" is from stuff prompt template\n",
    "# https://github.com/hwchase17/langchain/blob/master/langchain/chains/qa_with_sources/stuff_prompt.py#L31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f9ed2a-1e68-4b50-84a9-822bf08a1dc4",
   "metadata": {},
   "source": [
    "## Another way to QA using langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "64a75b81-f16f-4e11-aef4-9471d20bbfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8add11bb-0189-4ae1-8546-3f85c2416062",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = qa(\"What is attention?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b8d7f67e-2dd8-468e-b844-eb91734e9cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is attention?',\n",
       " 'result': \" Attention is a way to hold the state of the entire input by continuously performing weighted matrix multipli-cation that highlight the relevance of specific terms in relation to each other in the vocabulary. It's a very large, complex hash table that keeps track of the words in the text and how they map to different representations both in the input and the output.\",\n",
       " 'source_documents': [Document(page_content='a static set of outputs such as translated text or a text summary. In between\\nthe two types of layers is the attention mechanism , a way to hold the state\\nof the entire input by continuously performing weighted matrix multiplica-\\ntions that highlight the relevance of specific terms in relation to each other\\nin the vocabulary. We can think of attention as a very large, complex hash\\ntable that keeps track of the words in the text and how they map to different\\nrepresentations both in the input and the output.\\n-0.2\\n-0.1\\n0.1\\n0.4\\n-0.3\\n1.1Decoder Encoder DecoderTranslated\\ntextInput\\ntext\\nFigure 41: The encoder/decoder architecture\\n54', metadata={'source': 'embeddings.pdf', 'page': 53}),\n",
       "  Document(page_content='or the word encoding.\\nNext, these positional vectors are passed in parallel to the model. Within\\nthe Transformer paper, the model consists of six layers that perform encod-\\ning and six that perform decoding. We start with the encoder layer, which\\nconsists of two sub-layers: the self-attention layer, and a feed-forward neural\\nnetwork. The self-attention layer is the key piece, which performs the process\\nof learning the relationship of each term in relation to the other through scaled\\ndot-product attention. We can think of self-attention in several ways: as a\\ndifferentiable lookup table, or as a large lookup dictionary that contains both\\nthe terms and their positions, with the weights of each term in relationship to\\nthe other obtained from previous layers.\\nThe scaled dot-product attention is the product of three matrices: key,\\nquery, and value. These are initially all the same values that are outputs of\\nprevious layers - in the first pass through the model, they are initially all the', metadata={'source': 'embeddings.pdf', 'page': 56}),\n",
       "  Document(page_content='Once we have our lookup values, we can process all our words. For CBOW,\\nwe take a single word and we pick a sliding window, in our case, two words\\nbefore, and two words after, and try to infer what the actual word is. This is\\ncalled the context vector , and in other cases, we’ll see that it’s called attention.\\nFor example, if we have the phrase \"No bird [blank] too high\", we’re trying to\\npredict that the answer is \"soars\" with a given softmax probability, aka ranked\\nagainst other words. Once we have the context vector, we look at the loss —\\nthe difference between the true word and the predicted word as ranked by\\nprobability — and then we continue.\\nThe way we train this model is through context windows. For each given\\nword in the model, we create a sliding window that includes that word and 2\\nwords before it, and 2 words after it.\\nWe activate the linear layer with a ReLu activation function, which decides\\nwhether a given weight is important or not. In this case, ReLu squashes all the', metadata={'source': 'embeddings.pdf', 'page': 46})]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d0af25e1-b512-43c1-9aeb-911aa3b0c3f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Attention is a way to hold the state of the entire input by continuously performing weighted matrix multipli-cation that highlight the relevance of specific terms in relation to each other in the vocabulary. It's a very large, complex hash table that keeps track of the words in the text and how they map to different representations both in the input and the output.\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6abcd11c-8cba-43c4-a5c1-961a881106a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res['source_documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e1583d0-9bc3-40a3-bcfc-7c80d4bae744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='a static set of outputs such as translated text or a text summary. In between\\nthe two types of layers is the attention mechanism , a way to hold the state\\nof the entire input by continuously performing weighted matrix multiplica-\\ntions that highlight the relevance of specific terms in relation to each other\\nin the vocabulary. We can think of attention as a very large, complex hash\\ntable that keeps track of the words in the text and how they map to different\\nrepresentations both in the input and the output.\\n-0.2\\n-0.1\\n0.1\\n0.4\\n-0.3\\n1.1Decoder Encoder DecoderTranslated\\ntextInput\\ntext\\nFigure 41: The encoder/decoder architecture\\n54', metadata={'source': 'embeddings.pdf', 'page': 53})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['source_documents'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ddc96017-74eb-4715-a7d6-19ad0ea7aedb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'embeddings.pdf', 'page': 56}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['source_documents'][1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1cbb0ee5-74c4-4c55-810d-f520b12735ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "or the word encoding.\n",
       "Next, these positional vectors are passed in parallel to the model. Within\n",
       "the Transformer paper, the model consists of six layers that perform encod-\n",
       "ing and six that perform decoding. We start with the encoder layer, which\n",
       "consists of two sub-layers: the self-attention layer, and a feed-forward neural\n",
       "network. The self-attention layer is the key piece, which performs the process\n",
       "of learning the relationship of each term in relation to the other through scaled\n",
       "dot-product attention. We can think of self-attention in several ways: as a\n",
       "differentiable lookup table, or as a large lookup dictionary that contains both\n",
       "the terms and their positions, with the weights of each term in relationship to\n",
       "the other obtained from previous layers.\n",
       "The scaled dot-product attention is the product of three matrices: key,\n",
       "query, and value. These are initially all the same values that are outputs of\n",
       "previous layers - in the first pass through the model, they are initially all the"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(res['source_documents'][1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cd2abedf-d77a-4831-a930-49a7897a4ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Once we have our lookup values, we can process all our words. For CBOW,\\nwe take a single word and we pick a sliding window, in our case, two words\\nbefore, and two words after, and try to infer what the actual word is. This is\\ncalled the context vector , and in other cases, we’ll see that it’s called attention.\\nFor example, if we have the phrase \"No bird [blank] too high\", we’re trying to\\npredict that the answer is \"soars\" with a given softmax probability, aka ranked\\nagainst other words. Once we have the context vector, we look at the loss —\\nthe difference between the true word and the predicted word as ranked by\\nprobability — and then we continue.\\nThe way we train this model is through context windows. For each given\\nword in the model, we create a sliding window that includes that word and 2\\nwords before it, and 2 words after it.\\nWe activate the linear layer with a ReLu activation function, which decides\\nwhether a given weight is important or not. In this case, ReLu squashes all the', metadata={'source': 'embeddings.pdf', 'page': 46})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['source_documents'][2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
