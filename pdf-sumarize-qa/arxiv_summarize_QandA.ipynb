{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35589557-85b0-4fcd-8029-96c11766ae69",
   "metadata": {},
   "source": [
    "## A library to pull the paper content\n",
    "Arxiv is a library to pull the content of papers from Arxiv.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00336c6e-162a-4b57-b30c-d828d4f4ac08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qqq arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35e396dd-3682-4c8b-8faa-201f62eaf5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "import arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66fb8d06-f7f1-4a7b-9f50-35cf465762c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Is All You Need\n"
     ]
    }
   ],
   "source": [
    "search = arxiv.Search(id_list=[\"1706.03762\"])\n",
    "paper = next(search.results())\n",
    "print(paper.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e80ed978-7aae-4370-884b-2847af0234d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The dominant sequence transduction models are based on complex recurrent or\n",
       "convolutional neural networks in an encoder-decoder configuration. The best\n",
       "performing models also connect the encoder and decoder through an attention\n",
       "mechanism. We propose a new simple network architecture, the Transformer, based\n",
       "solely on attention mechanisms, dispensing with recurrence and convolutions\n",
       "entirely. Experiments on two machine translation tasks show these models to be\n",
       "superior in quality while being more parallelizable and requiring significantly\n",
       "less time to train. Our model achieves 28.4 BLEU on the WMT 2014\n",
       "English-to-German translation task, improving over the existing best results,\n",
       "including ensembles by over 2 BLEU. On the WMT 2014 English-to-French\n",
       "translation task, our model establishes a new single-model state-of-the-art\n",
       "BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\n",
       "of the training costs of the best models from the literature. We show that the\n",
       "Transformer generalizes well to other tasks by applying it successfully to\n",
       "English constituency parsing both with large and limited training data."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(paper.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d4011ba-5a35-4bff-8f12-ab6d0d00e2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qqq langchain openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81c339d-7909-4a30-a8a1-b611168a7e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "openai_api_key = getpass.getpass(\"OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3367302-7413-4c0f-bb90-70e61222f2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1dd2f7c3",
   "metadata": {},
   "source": [
    "## text-davinci-003 vs gpt-3.5-turbo\n",
    "\n",
    "text-davinci-003 and gpt-3.5-turbo are two different language models created by OpenAI. The main differences are:\n",
    "\n",
    "text-davinci-003:\n",
    "\n",
    "    It is a newer and larger model than gpt-3.5-turbo, trained on more data.\n",
    "    It has 9.4 billion parameters, compared to 3.5 billion for gpt-3.5-turbo. More parameters usually means the model can capture more complex patterns.\n",
    "    It achieves state-of-the-art results on many language tasks like summarization, question answering, word sense disambiguation, etc.\n",
    "    It has a wider range of capabilities, covering both conversational and formal language.\n",
    "\n",
    "gpt-3.5-turbo:\n",
    "\n",
    "    It is an earlier and smaller model from OpenAI, launched in 2020.\n",
    "    It has fewer parameters so may generate less coherent long-form text, but can still perform many language tasks reasonably well.\n",
    "    It is optimized more for conversational language, so may be better at casual dialogue and humor.\n",
    "    It tends to have a more casual and colloquial tone in its writing style.\n",
    "\n",
    "In summary, text-davinci-003 is a more advanced, higher-capacity model that outperforms gpt-3.5-turbo on most language tasks. However, gpt-3.5-turbo can still be useful for some applications where a more informal or conversational style is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0ed7e5a-b21a-4117-8dad-b7672f576479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a12398d-4888-4aff-bb01-de6663fb2cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "572539f8-24bb-4703-bc34-1b4891323724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Hello! It looks like you are testing something. What is the purpose of your test?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(llm(\"hello, this is a test\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3952c01-7bab-4482-a2c3-91dcfa3549cd",
   "metadata": {},
   "source": [
    "## Using langchain and OpenAI to get the summary of the paper by giving context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebaaebfa-9780-4dda-a339-94de1527600e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Attention is a mechanism that connects the encoder and decoder in sequence transduction models. It helps the model to focus on specific parts of the input when making predictions and can improve the performance of the model."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm(f\"\"\"Here's a summary of a paper:\n",
    "{paper.summary}\n",
    "\n",
    "Based on that summary, what is attention?\"\"\")\n",
    "\n",
    "Markdown(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88f27d39-957d-458b-9c61-5f27348f0644",
   "metadata": {},
   "source": [
    "## pypdf to load pdf content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "639c6e8a-bd8d-45c3-bae7-7ceab266aa83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qqq pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "717e4a5f-04dc-4d70-ba66-f11c96976dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_path = paper.download_pdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1387c387-9e84-4d57-a182-45b1183934da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./1706.03762v5.Attention_Is_All_You_Need.pdf'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19649c3b-9cdd-45b1-9dd6-9b0b218110d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(paper_path)\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3265dd4-d5dc-4486-b1e1-f5e7a526a8d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57b506c7-67d6-457a-bbbe-c933e7a85b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"\\n\\n\".join([page.page_content for page in pages[0:2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c42d2a01-c191-40e2-a6d3-55b740a36b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Attention Is All You Need\n",
       "Ashish Vaswani\u0003\n",
       "Google Brain\n",
       "avaswani@google.comNoam Shazeer\u0003\n",
       "Google Brain\n",
       "noam@google.comNiki Parmar\u0003\n",
       "Google Research\n",
       "nikip@google.comJakob Uszkoreit\u0003\n",
       "Google Research\n",
       "usz@google.com\n",
       "Llion Jones\u0003\n",
       "Google Research\n",
       "llion@google.comAidan N. Gomez\u0003y\n",
       "University of Toronto\n",
       "aidan@cs.toronto.eduŁukasz Kaiser\u0003\n",
       "Google Brain\n",
       "lukaszkaiser@google.com\n",
       "Illia Polosukhin\u0003z\n",
       "illia.polosukhin@gmail.com\n",
       "Abstract\n",
       "The dominant sequence transduction models are based on complex recurrent or\n",
       "convolutional neural networks that include an encoder and a decoder. The best\n",
       "performing models also connect the encoder and decoder through an attention\n",
       "mechanism. We propose a new simple network architecture, the Transformer,\n",
       "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
       "entirely. Experiments on two machine translation tasks show these models to\n",
       "be superior in quality while being more parallelizable and requiring signiﬁcantly\n",
       "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
       "to-German translation task, improving over the existing best results, including\n",
       "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
       "our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
       "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
       "best models from the literature. We show that the Transformer generalizes well to\n",
       "other tasks by applying it successfully to English constituency parsing both with\n",
       "large and limited training data.\n",
       "1 Introduction\n",
       "Recurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\n",
       "in particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\n",
       "\u0003Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
       "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\n",
       "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
       "attention and the parameter-free position representation and became the other person involved in nearly every\n",
       "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
       "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\n",
       "efﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
       "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\n",
       "our research.\n",
       "yWork performed while at Google Brain.\n",
       "zWork performed while at Google Research.\n",
       "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v5  [cs.CL]  6 Dec 2017\n",
       "\n",
       "transduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\n",
       "efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\n",
       "architectures [38, 24, 15].\n",
       "Recurrent models typically factor computation along the symbol positions of the input and output\n",
       "sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\n",
       "statesht, as a function of the previous hidden state ht\u00001and the input for position t. This inherently\n",
       "sequential nature precludes parallelization within training examples, which becomes critical at longer\n",
       "sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\n",
       "signiﬁcant improvements in computational efﬁciency through factorization tricks [ 21] and conditional\n",
       "computation [ 32], while also improving model performance in case of the latter. The fundamental\n",
       "constraint of sequential computation, however, remains.\n",
       "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
       "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
       "the input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\n",
       "are used in conjunction with a recurrent network.\n",
       "In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
       "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
       "The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\n",
       "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
       "2 Background\n",
       "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
       "[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\n",
       "block, computing hidden representations in parallel for all input and output positions. In these models,\n",
       "the number of operations required to relate signals from two arbitrary input or output positions grows\n",
       "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
       "it more difﬁcult to learn dependencies between distant positions [ 12]. In the Transformer this is\n",
       "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
       "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
       "described in section 3.2.\n",
       "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
       "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
       "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
       "textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
       "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
       "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
       "language modeling tasks [34].\n",
       "To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\n",
       "entirely on self-attention to compute representations of its input and output without using sequence-\n",
       "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
       "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
       "3 Model Architecture\n",
       "Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\n",
       "Here, the encoder maps an input sequence of symbol representations (x1;:::;x n)to a sequence\n",
       "of continuous representations z= (z1;:::;z n). Given z, the decoder then generates an output\n",
       "sequence (y1;:::;y m)of symbols one element at a time. At each step the model is auto-regressive"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f8806b6-fcb3-4889-a1f9-445dc30770a4",
   "metadata": {},
   "source": [
    "## You can ask question on above pdf like below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bd7f258d-93c8-4764-a666-070b54d028af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Multi-head attention is a mechanism used in the Transformer model architecture which allows the model to draw global dependencies between input and output. It works by splitting the model's attention into multiple \"heads\" which each focus on different parts of the input and output sequence, thus allowing the model to process more information at once and improve its performance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm(f\"\"\"Here's the first two pages of a paper:\n",
    "{content}\n",
    "\n",
    "Based on that content, what is multi-head attention?\"\"\")\n",
    "\n",
    "Markdown(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2f45ae1-3728-47a0-bc4d-b408a098cc6e",
   "metadata": {},
   "source": [
    "## Now we are using \n",
    "### FAISS\n",
    "FAISS is a library for efficient similarity search and clustering of dense vectors, but it doesn't come preloaded with any data. You need to provide your own data in the form of vectors to perform similarity search using FAISS.\n",
    "\n",
    "The data can be loaded into FAISS from various sources, such as local files, databases, or remote APIs, depending on the specific use case and the data storage requirements. Once the data is loaded into FAISS, it can be indexed and searched efficiently using the various indexing methods provided by the library.\n",
    "\n",
    "FAISS is designed to work with large-scale datasets, so it can handle large amounts of data efficiently. It can also be used for real-time search by keeping the index in memory and updating it in real-time as new data becomes available.\n",
    "\n",
    "Overall, FAISS relies on the data that is provided to it locally or from other sources, and it doesn't have any built-in mechanism to pull data from a real-time vector database for similarity search. However, it can be integrated with other tools and systems to access data from remote sources and perform real-time indexing and search.\n",
    "\n",
    "### tiktoken\n",
    "tiktoken is a fast BPE tokeniser for use with OpenAI's models.\n",
    "\n",
    "Models don't see text like you and I, instead they see a sequence of numbers (known as tokens). Byte pair encoding (BPE) is a way of converting text into tokens. It has a couple desirable properties: It's reversible and lossless, so you can convert tokens back into the original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2aa7ffb4-5842-4a77-a9f3-8be304a84459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qqq faiss-cpu tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a2caf300-ea05-46c7-867f-b32502d7062b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "299dcd7d-62ff-4e03-baf8-ecae21154c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d627c6d1-729f-491e-a003-193aaf7bb215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "43103378-3efd-44cb-b4d5-78532dba1250",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8fe92cd-57bb-4999-9d18-d59bfa131793",
   "metadata": {},
   "source": [
    "## Below are feeding the pdf content and the OpenAI embeddings to FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f9d8836-13b6-4572-9506-6bb1f3ef7d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "63cba3f6-1e8b-4d29-87e5-fff7dbf87740",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db.similarity_search(\"What is attention?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a329ee4a-1a97-4379-8b7f-906e68a131f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Attention Visualizations\n",
       "Input-Input Layer5\n",
       "It\n",
       "is\n",
       "in\n",
       "this\n",
       "spirit\n",
       "that\n",
       "a\n",
       "majority\n",
       "of\n",
       "American\n",
       "governments\n",
       "have\n",
       "passed\n",
       "new\n",
       "laws\n",
       "since\n",
       "2009\n",
       "making\n",
       "the\n",
       "registration\n",
       "or\n",
       "voting\n",
       "process\n",
       "more\n",
       "difficult\n",
       ".\n",
       "<EOS>\n",
       "<pad>\n",
       "<pad>\n",
       "<pad>\n",
       "<pad>\n",
       "<pad>\n",
       "<pad>\n",
       "It\n",
       "is\n",
       "in\n",
       "this\n",
       "spirit\n",
       "that\n",
       "a\n",
       "majority\n",
       "of\n",
       "American\n",
       "governments\n",
       "have\n",
       "passed\n",
       "new\n",
       "laws\n",
       "since\n",
       "2009\n",
       "making\n",
       "the\n",
       "registration\n",
       "or\n",
       "voting\n",
       "process\n",
       "more\n",
       "difficult\n",
       ".\n",
       "<EOS>\n",
       "<pad>\n",
       "<pad>\n",
       "<pad>\n",
       "<pad>\n",
       "<pad>\n",
       "<pad>\n",
       "Figure 3: An example of the attention mechanism following long-distance dependencies in the\n",
       "encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\n",
       "the verb ‘making’, completing the phrase ‘making...more difﬁcult’. Attentions here shown only for\n",
       "the word ‘making’. Different colors represent different heads. Best viewed in color.\n",
       "13"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(docs[0].page_content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "975aa2f8-fc98-4942-9d7e-f419e2618442",
   "metadata": {},
   "source": [
    "## using load_qa_with_sources_chain\n",
    "\n",
    "The load_qa_with_sources_chain() method in the langchain library is used to load question-answer pairs along with their source documents. It retrieves questions, answers and source documents from a data source and prepares them for processing by the langchain library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fc3a74c7-781c-4ca0-aea6-131eca6e52fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "221c82d7-4f57-47df-b07a-cd4706e201a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\")\n",
    "query = \"What is attention?\"\n",
    "sources = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d5bef5e6-f342-4ff5-bbdb-61a7c6ecf2ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difﬁcult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13', metadata={'source': './1706.03762v5.Attention_Is_All_You_Need.pdf', 'page': 12})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "faefd71a-3a39-40b4-9f4a-15d1ffe03c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = chain({\"input_documents\": sources, \"question\": query}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d2f540fb-55de-478b-867e-2e2dc80add64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Attention is an integral part of sequence modeling and transduction models that allow for modeling of dependencies without regard to their distance in the input or output sequences.\n",
       "SOURCES: 1706.03762v5.Attention_Is_All_You_Need.pdf"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(results['output_text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02fdd4a8-8cc7-46a0-8f3d-43ab1cb9a521",
   "metadata": {},
   "source": [
    "### Above is showing the source, from where did it get the answer for the given question. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
